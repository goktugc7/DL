{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense  \n",
    "from keras import applications  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import math  \n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images:  2013\n",
      "Training:  1409\n",
      "Validation:  302\n",
      "Testing:  302\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "src = 'animals10/raw-img'\n",
    "current = '/scoiattolo'\n",
    "src = src + current\n",
    "files = os.listdir(src)\n",
    "np.random.shuffle(files)\n",
    "\n",
    "train_files, val_files, test_files = np.split(np.array(files), [int(.7*len(files)), int(.85*len(files))])\n",
    "\n",
    "train_files = [src + '/' + name for name in train_files.tolist()]\n",
    "val_files = [src + '/' + name for name in val_files.tolist()]\n",
    "test_files = [src + '/' + name for name in test_files.tolist()]\n",
    "\n",
    "print('Total images: ', len(files))\n",
    "print('Training: ', len(train_files))\n",
    "print('Validation: ', len(val_files))\n",
    "print('Testing: ', len(test_files))\n",
    "\n",
    "# Copy-pasting images\n",
    "for name in train_files:\n",
    "    shutil.copy(name, \"animals10/training\"+current)\n",
    "\n",
    "for name in val_files:\n",
    "    shutil.copy(name, \"animals10/validation\"+current)\n",
    "\n",
    "for name in test_files:\n",
    "    shutil.copy(name, \"animals10/testing\"+current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19782 images belonging to 10 classes.\n",
      "Time:  1:25:54.386101\n"
     ]
    }
   ],
   "source": [
    "training_folder_data = 'animals10/training'\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 50\n",
    "vgg16 = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "generator = datagen.flow_from_directory(  \n",
    "     training_folder_data,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator.filenames)  \n",
    "num_classes = len(generator.class_indices)  \n",
    "   \n",
    "predict_size_training = int(math.ceil(nb_train_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_training = vgg16.predict_generator(generator, predict_size_training)  \n",
    "   \n",
    "np.save('bottleneck_features_training.npy', bottleneck_features_training)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4240 images belonging to 10 classes.\n",
      "Time:  0:18:40.140978\n"
     ]
    }
   ],
   "source": [
    "validation_folder_data = 'animals10/validation'\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "generator = datagen.flow_from_directory(  \n",
    "     validation_folder_data,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator.filenames)  \n",
    "num_classes = len(generator.class_indices)  \n",
    "   \n",
    "predict_size_val = int(math.ceil(nb_train_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_val = vgg16.predict_generator(generator, predict_size_val)  \n",
    "   \n",
    "np.save('bottleneck_features_validation.npy', bottleneck_features_val)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4244 images belonging to 10 classes.\n",
      "Time:  0:18:47.623570\n"
     ]
    }
   ],
   "source": [
    "testing_folder_data = 'animals10/testing'\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "   \n",
    "generator = datagen.flow_from_directory(  \n",
    "     testing_folder_data,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator.filenames)  \n",
    "num_classes = len(generator.class_indices)  \n",
    "   \n",
    "predict_size_testing = int(math.ceil(nb_train_samples / batch_size))  \n",
    "   \n",
    "bottleneck_features_testing = vgg16.predict_generator(generator, predict_size_testing)  \n",
    "   \n",
    "np.save('bottleneck_features_testing.npy', bottleneck_features_testing)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19782 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         training_folder_data,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "   \n",
    "# load the bottleneck features saved earlier  \n",
    "train_data = np.load('bottleneck_features_training.npy')  \n",
    "   \n",
    "# get the class lebels for the training data, in the original order  \n",
    "train_labels = generator_top.classes  \n",
    "   \n",
    "# convert the training labels to categorical vectors  \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4240 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#validation data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         validation_folder_data,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "   \n",
    "# load the bottleneck features saved earlier  \n",
    "validation_data = np.load('bottleneck_features_validation.npy')  \n",
    "   \n",
    "# get the class lebels for the training data, in the original order  \n",
    "validation_labels = generator_top.classes  \n",
    "   \n",
    "# convert the training labels to categorical vectors  \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4244 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "#testing data\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "         testing_folder_data,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "   \n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "   \n",
    "# load the bottleneck features saved earlier  \n",
    "testing_data = np.load('bottleneck_features_testing.npy')  \n",
    "   \n",
    "# get the class lebels for the training data, in the original order  \n",
    "testing_labels = generator_top.classes  \n",
    "   \n",
    "# convert the training labels to categorical vectors  \n",
    "testing_labels = to_categorical(testing_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19782 samples, validate on 4240 samples\n",
      "Epoch 1/7\n",
      "19782/19782 [==============================] - 21s 1ms/step - loss: 1.2376 - acc: 0.5863 - val_loss: 0.7904 - val_acc: 0.7606\n",
      "Epoch 2/7\n",
      "19782/19782 [==============================] - 20s 1ms/step - loss: 0.8171 - acc: 0.7379 - val_loss: 0.6201 - val_acc: 0.8042\n",
      "Epoch 3/7\n",
      "19782/19782 [==============================] - 20s 1ms/step - loss: 0.6794 - acc: 0.7808 - val_loss: 0.5672 - val_acc: 0.8137\n",
      "Epoch 4/7\n",
      "19782/19782 [==============================] - 21s 1ms/step - loss: 0.5943 - acc: 0.8065 - val_loss: 0.5415 - val_acc: 0.8238\n",
      "Epoch 5/7\n",
      "19782/19782 [==============================] - 21s 1ms/step - loss: 0.5188 - acc: 0.8331 - val_loss: 0.5012 - val_acc: 0.8408\n",
      "Epoch 6/7\n",
      "19782/19782 [==============================] - 21s 1ms/step - loss: 0.4746 - acc: 0.8456 - val_loss: 0.4874 - val_acc: 0.8472\n",
      "Epoch 7/7\n",
      "19782/19782 [==============================] - 21s 1ms/step - loss: 0.4333 - acc: 0.8607 - val_loss: 0.4870 - val_acc: 0.8415\n",
      "4240/4240 [==============================] - 1s 181us/step\n",
      "[INFO] accuracy: 84.15%\n",
      "[INFO] Loss: 0.48701815031258\n",
      "Time:  0:02:26.762365\n"
     ]
    }
   ],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "start = datetime.datetime.now()\n",
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))  \n",
    "model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3)))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3)))  \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])  \n",
    "\n",
    "history = model.fit(train_data, train_labels,  \n",
    "      epochs=7,\n",
    "      batch_size=batch_size,  \n",
    "      validation_data=(validation_data, validation_labels))  \n",
    "\n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5' \n",
    "model.save_weights(top_model_weights_path)  \n",
    "\n",
    "(eval_loss, eval_accuracy) = model.evaluate(  \n",
    " validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss))  \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
